{
  "metadata": {
    "name": "imdb-emotion-analysis",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val basePath \u003d \"/user/yc7093_nyu_edu/imdb-reviews-w-emotion/part\"\nval fileSuffixes \u003d List(\"-01-all\") //, \"-02-all\", \"-03-all\", \"-04-all\")\n\n// Initialize the first DataFrame with the schema of the first file\nval initialPath \u003d s\"$basePath${fileSuffixes.head}\"\nvar rawDF \u003d spark.read.parquet(initialPath)\n\n// Loop through the remaining file suffixes, construct the full path, and concatenate the DataFrames\nfor (suffix \u003c- fileSuffixes.tail) {\n  val fullPath \u003d s\"$basePath$suffix\" // Construct the full path\n  val part_df \u003d spark.read.parquet(fullPath) // Read each Parquet file\n  rawDF \u003d rawDF.union(part_df) // Concatenate the DataFrames\n}\n\n// Show the result\nrawDF.show(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Assuming the ratings are stored in a column named \"rating\"\n// Replace \"rating\" with the actual column name in your DataFrame\nval ratingDistribution \u003d rawDF.groupBy(\"rating\")\n  .count() // Count the number of occurrences for each rating\n  .orderBy(\"rating\") // Optional: Order by rating for better readability\n\n// Show the result\nratingDistribution.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d \"/user/yc7093_nyu_edu/imdb-emotion-analysis/rating-distribution\"\n\n// Coalesce to a single partition and write as CSV\nratingDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// Assuming the emotions are stored in a column named \"emotion\"\n// Replace \"emotion\" with the actual column name in your DataFrame\nval emotionDistribution \u003d rawDF.groupBy(\"emotion\")\n  .count() // Count the number of occurrences for each emotion\n  .orderBy(\"emotion\") // Optional: Order by emotion for better readability\n\n// Show the result\nemotionDistribution.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d \"/user/yc7093_nyu_edu/imdb-emotion-analysis/emotion-distribution\"\n\n// Coalesce to a single partition and write as CSV\nemotionDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\n\n// Define a window partitioned by emotion\nval emotionWindow \u003d Window.partitionBy(\"emotion\")\n\n// Group by emotion and rating to get the count\nval ratingDistributionWithinEmotion \u003d rawDF.groupBy(\"emotion\", \"rating\")\n  .count() // Count the number of occurrences for each emotion and rating\n  .withColumn(\"total_count\", sum(\"count\").over(emotionWindow)) // Total count per emotion\n  .withColumn(\"percentage\", (col(\"count\") / col(\"total_count\")) * 100) // Percentage calculation\n  .orderBy(\"emotion\", \"rating\") // Order by emotion and then by rating\n\n// Show the result\nval rowCount \u003d ratingDistributionWithinEmotion.count()\nratingDistributionWithinEmotion.show(rowCount.toInt, truncate \u003d false)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d \"/user/yc7093_nyu_edu/imdb-emotion-analysis/rating-distribution-w-emotion\"\n\n// Coalesce to a single partition and write as CSV\nratingDistributionWithinEmotion.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\n\n// Define a window partitioned by rating\nval ratingWindow \u003d Window.partitionBy(\"rating\")\n\n// Group by rating and emotion to get the count\nval emotionDistributionWithinRating \u003d rawDF.groupBy(\"rating\", \"emotion\")\n  .count() // Count the number of occurrences for each rating and emotion\n  .withColumn(\"total_count\", sum(\"count\").over(ratingWindow)) // Total count per rating\n  .withColumn(\"percentage\", (col(\"count\") / col(\"total_count\")) * 100) // Percentage calculation\n  .orderBy(\"rating\", \"emotion\") // Order by rating and then by emotion\n\n// Show the result\nval rowCount \u003d emotionDistributionWithinRating.count()\nemotionDistributionWithinRating.show(rowCount.toInt, truncate \u003d false)\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d \"/user/yc7093_nyu_edu/imdb-emotion-analysis/emotion-distribution-w-rating\"\n\n// Coalesce to a single partition and write as CSV\nemotionDistributionWithinRating.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Define the base path and the emotion you want to analyze\nval basePath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word\"\nval specificEmotion \u003d \"sadness\"\n\n// Load the partitioned data for the specific emotion\nval emotionDf \u003d spark.read.parquet(s\"$basePath/emotion\u003d$specificEmotion\")\n\n// Calculate keyword distribution\nval keywordDistribution \u003d emotionDf.groupBy(\"word\")\n  .count() // Count occurrences of each word within the emotion\n  .orderBy(desc(\"count\")) // Order by count in descending order\n\n\n\n// Show the result\nval rowCount \u003d keywordDistribution.count()\nkeywordDistribution.show(rowCount.toInt, truncate \u003d false)\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d s\"/user/yc7093_nyu_edu/imdb-emotion-analysis/$specificEmotion-keyword-distribution\"\n\n// Coalesce to a single partition and write as CSV\nkeywordDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Define the base path and the emotion you want to analyze\nval basePath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word\"\nval specificEmotion \u003d \"love\"\n\n// Load the partitioned data for the specific emotion\nval emotionDf \u003d spark.read.parquet(s\"$basePath/emotion\u003d$specificEmotion\")\n\n// Calculate keyword distribution\nval keywordDistribution \u003d emotionDf.groupBy(\"word\")\n  .count() // Count occurrences of each word within the emotion\n  .orderBy(desc(\"count\")) // Order by count in descending order\n\n\n// Show the result\nval rowCount \u003d keywordDistribution.count()\nkeywordDistribution.show(rowCount.toInt, truncate \u003d false)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d s\"/user/yc7093_nyu_edu/imdb-emotion-analysis/$specificEmotion-keyword-distribution\"\n\n// Coalesce to a single partition and write as CSV\nkeywordDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Define the base path and the emotion you want to analyze\nval basePath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word\"\nval specificEmotion \u003d \"joy\"\n\n// Load the partitioned data for the specific emotion\nval emotionDf \u003d spark.read.parquet(s\"$basePath/emotion\u003d$specificEmotion\")\n\n// Calculate keyword distribution\nval keywordDistribution \u003d emotionDf.groupBy(\"word\")\n  .count() // Count occurrences of each word within the emotion\n  .orderBy(desc(\"count\")) // Order by count in descending order\n\n\n// Show the result\nval rowCount \u003d keywordDistribution.count()\nkeywordDistribution.show(rowCount.toInt, truncate \u003d false)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d s\"/user/yc7093_nyu_edu/imdb-emotion-analysis/$specificEmotion-keyword-distribution\"\n\n// Coalesce to a single partition and write as CSV\nkeywordDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Define the base path and the emotion you want to analyze\nval basePath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word\"\nval specificEmotion \u003d \"surprise\"\n\n// Load the partitioned data for the specific emotion\nval emotionDf \u003d spark.read.parquet(s\"$basePath/emotion\u003d$specificEmotion\")\n\n// Calculate keyword distribution\nval keywordDistribution \u003d emotionDf.groupBy(\"word\")\n  .count() // Count occurrences of each word within the emotion\n  .orderBy(desc(\"count\")) // Order by count in descending order\n\n\n// Show the result\nval rowCount \u003d keywordDistribution.count()\nkeywordDistribution.show(rowCount.toInt, truncate \u003d false)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d s\"/user/yc7093_nyu_edu/imdb-emotion-analysis/$specificEmotion-keyword-distribution\"\n\n// Coalesce to a single partition and write as CSV\nkeywordDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Define the base path and the emotion you want to analyze\nval basePath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word\"\nval specificEmotion \u003d \"anger\"\n\n// Load the partitioned data for the specific emotion\nval emotionDf \u003d spark.read.parquet(s\"$basePath/emotion\u003d$specificEmotion\")\n\n// Calculate keyword distribution\nval keywordDistribution \u003d emotionDf.groupBy(\"word\")\n  .count() // Count occurrences of each word within the emotion\n  .orderBy(desc(\"count\")) // Order by count in descending order\n\n\n// Show the result\nval rowCount \u003d keywordDistribution.count()\nkeywordDistribution.show(rowCount.toInt, truncate \u003d false)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d s\"/user/yc7093_nyu_edu/imdb-emotion-analysis/$specificEmotion-keyword-distribution\"\n\n// Coalesce to a single partition and write as CSV\nkeywordDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Define the base path and the emotion you want to analyze\nval basePath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word\"\nval specificEmotion \u003d \"fear\"\n\n// Load the partitioned data for the specific emotion\nval emotionDf \u003d spark.read.parquet(s\"$basePath/emotion\u003d$specificEmotion\")\n\n// Calculate keyword distribution\nval keywordDistribution \u003d emotionDf.groupBy(\"word\")\n  .count() // Count occurrences of each word within the emotion\n  .orderBy(desc(\"count\")) // Order by count in descending order\n\n\n// Show the result\nval rowCount \u003d keywordDistribution.count()\nkeywordDistribution.show(rowCount.toInt, truncate \u003d false)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d s\"/user/yc7093_nyu_edu/imdb-emotion-analysis/$specificEmotion-keyword-distribution\"\n\n// Coalesce to a single partition and write as CSV\nkeywordDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\n\n// Define the base path for the partitioned data\nval basePath \u003d \"/user/yc7093_nyu_edu/imdb_partitioned_by_emotion_rating_word\"\n\n// Read the partitioned Parquet data\nval partitionedDf \u003d spark.read.parquet(basePath)\n\n// Group by \"word\" and \"rating\" to calculate the count for each combination\nval ratingDistributionByKeyword \u003d partitionedDf.groupBy(\"word\", \"rating\")\n  .count() // Count occurrences for each word-rating combination\n  .orderBy(\"word\", \"rating\") // Optional: Order by word and rating for better readability\n\n// Pivot the table to make \"word\" the row index and \"rating\" the column index\nval pivotedDistribution \u003d ratingDistributionByKeyword.groupBy(\"word\")\n  .pivot(\"rating\") // Pivot on the \"rating\" column\n  .sum(\"count\") // Aggregate counts for each word-rating combination\n\n// Show the resulting DataFrame\npivotedDistribution.show(truncate \u003d false)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d \"/user/yc7093_nyu_edu/imdb-emotion-analysis/rating-distribution-w-word\"\n\n// Coalesce to a single partition and write as CSV\npivotedDistribution.coalesce(1)\n  .write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n"
    }
  ]
}